{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXCEED Python Qualtrics Survey - High-Level Analysis\n",
    "\n",
    "This document provides a high-level analysis of the results from the Qualtrics Python Skill Level Survey. The goal of the survey was to objectively assess the Python skill levels of participants, by placing them into one of the 2 categories: _Beginner_/_Novices_ and _Advanced_/_Experts_. \n",
    "\n",
    "The ultimate goal is to identify the best subset of questions that can effectively differentiate between these two skill levels. The actual content of the survey is centered around several aspects of Python programming error messages, and debugging techniques. The underlying assumption is that individuals with more experience in Python (and perhaps even general programming experience) will have a better understanding of error identification, resolution, and debugging strategies.\n",
    "\n",
    "The actual metrics that are captured in this analysis include:\n",
    "- **Completion Rate**: The percentage of participants who started and completed the survey.\n",
    "- **Average duration**: The average time (in seconds) taken by participants to complete the survey.\n",
    "- **Median duration**: The median time (in seconds) taken by participants to complete the survey.\n",
    "- **Average Python YoE**: The average number of years of experience participants have with Python.\n",
    "- **Average Programming YoE**: The average number of years of experience participants have with programming in general.\n",
    "- **Average Self-reported score**: The average self-reported score of correctly answered questions (out of 16 questions).\n",
    "- **Correlation between Python YoE and Programming YoE**: The Pearson correlation coefficient between the number of years of experience with Python and the number of years of experience with programming in general.\n",
    "- **Correlation between self-reported score and Python YoE**: The Spearman correlation coefficient between the self-reported correct answers score and the number of years of experience with Python.\n",
    "- **Correlation between self-reported score and Programming YoE**: The Spearman correlation coefficient between the self-reported correct answers score and the number of years of experience with programming in general.\n",
    "\n",
    "> Notes: \n",
    "> - Since Python YoE and Programming YoE are essentially continuous variables, the correlation coefficients are calculated using the Pearson method.\n",
    "> - The self-reported number of correctly answered questions is a discrete variable (0 - 16), so the Spearman method is used for correlation calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the path to the Qualtrics CSV file (adjust as needed, but default is in the same directory)\n",
    "file_path = \"./qualtrics_responses_labels_simplified.csv\"\n",
    "\n",
    "# 1. Load data (Qualtrics exports are semicolon‑delimited by default)\n",
    "df = pd.read_csv(file_path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install Required Libraries\n",
    "\n",
    "Prior to running the analysis, ensure that the required libraries are installed. For this, you need only install the libraries defined in the `requirements.txt` file. You can do this by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "We need to load the survey response data from a CSV file, prior to performing any sort of analysis. Default file name should not be changed unless you have a different file name for the survey results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the path to the Qualtrics CSV file (adjust as needed, but default is in the same directory)\n",
    "file_path = \"./qualtrics_responses_labels_simplified.csv\"\n",
    "\n",
    "# 1. Load data (Qualtrics exports are semicolon‑delimited by default)\n",
    "df = pd.read_csv(file_path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Keep Only Completed Surveys\n",
    "\n",
    "In the dataset, it is possible to encounter entries of respondents that have started but not completed the survey within the allowed 2 weeks timeframe on Qualtrics. The following step filters out these incomplete surveys, ensuring that only fully completed responses are analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Keep only respondents who finished the survey\n",
    "completed = df[df[\"Finished\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze the Data\n",
    "\n",
    "This step calculates the metrics defined in the introduction to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate the required metrics\n",
    "metrics = {\n",
    "    \"Total number of responses\": len(df),\n",
    "    \"Total number of completed responses\": len(completed),\n",
    "    \"Completion rate (%)\": round(df[\"Finished\"].mean() * 100, 2),\n",
    "    \"Average duration (sec)\": completed[\"Duration (in seconds)\"].mean(),\n",
    "    \"Median duration (sec)\": completed[\"Duration (in seconds)\"].median(),\n",
    "    \"Average Python experience (years)\": completed[\"Q2.2\"].mean(),\n",
    "    \"Average general experience (years)\": completed[\"Q2.3\"].mean(),\n",
    "    \"Average estimated correct answers\": completed[\"Q11.1\"].mean(),\n",
    "    # Some correlations\n",
    "    \"Pearson Correlation (Python YoE and General programming YoE)\": completed[[\"Q2.2\", \"Q2.3\"]].corr().iloc[0, 1],\n",
    "    \"Spearman Correlation (Python YoE and self-reported score)\": completed[[\"Q2.2\", \"Q11.1\"]].corr(method='spearman').iloc[0, 1],\n",
    "    \"Spearman Correlation (General programming YoE and self-reported score)\": completed[[\"Q2.3\", \"Q11.1\"]].corr(method='spearman').iloc[0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate the Report\n",
    "\n",
    "This step creates a summary report of the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 4. Present results in a tidy table\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m summary_df = \u001b[43mpd\u001b[49m.DataFrame(\u001b[38;5;28mlist\u001b[39m(metrics.items()), columns=[\u001b[33m\"\u001b[39m\u001b[33mMetric\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mValue\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      6\u001b[39m display(summary_df.style.set_caption(\u001b[33m\"\u001b[39m\u001b[33mQualtrics Python Skill Level Assessment - High-level Metrics\u001b[39m\u001b[33m\"\u001b[39m).format(precision=\u001b[32m2\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# 4. Present results in a tidy table\n",
    "summary_df = pd.DataFrame(list(metrics.items()), columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "display(summary_df.style.set_caption(\"Qualtrics Python Skill Level Assessment - High-level Metrics\").format(precision=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
